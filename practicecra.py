# -*- coding: utf-8 -*-
"""practiceCRA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KDGQXA7CAv0ytpuxxAhiEsTuLMCPtyNL
"""

from google.colab import files
import pandas as pd

# Upload the dataset
uploaded = files.upload()

# Load the dataset (assuming the file is named 'credit_risk_dataset.csv')
data = pd.read_csv('credit_risk_dataset.csv')

# Display the first few rows of the dataset
data.head()

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Separate features and target
X = data.drop('loan_status', axis=1)
y = data['loan_status']

# Categorical and numerical columns
cat_cols = ['person_home_ownership', 'loan_intent', 'loan_grade', 'cb_person_default_on_file']
num_cols = ['person_age', 'person_income', 'person_emp_length', 'loan_amnt', 'loan_int_rate', 'loan_percent_income', 'cb_person_cred_hist_length']

# Preprocessing for numerical data
num_pipeline = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

# Preprocessing for categorical data
cat_pipeline = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Combine pipelines into a single ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', num_pipeline, num_cols),
        ('cat', cat_pipeline, cat_cols)
    ])

# Apply preprocessing
X_preprocessed = preprocessor.fit_transform(X)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.3, random_state=42)

!pip install -U imbalanced-learn

from imblearn.over_sampling import SMOTE

# Handle Data Imbalance using SMOTE
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier

# Logistic Regression
log_reg = LogisticRegression(random_state=42)
log_reg.fit(X_train_resampled, y_train_resampled)

# Random Forest
rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)
rf_clf.fit(X_train_resampled, y_train_resampled)

from sklearn.metrics import roc_auc_score, classification_report

# Predict on test set
y_pred_log_reg = log_reg.predict(X_test)
y_pred_rf_clf = rf_clf.predict(X_test)

# AUC-ROC Scores
auc_log_reg = roc_auc_score(y_test, log_reg.predict_proba(X_test)[:, 1])
auc_rf_clf = roc_auc_score(y_test, rf_clf.predict_proba(X_test)[:, 1])

# Classification Reports
log_reg_report = classification_report(y_test, y_pred_log_reg)
rf_clf_report = classification_report(y_test, y_pred_rf_clf)

# Print evaluation results
print("Logistic Regression AUC-ROC:", auc_log_reg)
print("Random Forest AUC-ROC:", auc_rf_clf)
print("Classification Report (Logistic Regression):\n", log_reg_report)
print("Classification Report (Random Forest):\n", rf_clf_report)

import numpy as np

# Probability of Default (PD)
y_prob = rf_clf.predict_proba(X_test)[:, 1]

# Define risk categories
risk_categories = np.where(y_prob < 0.3, 'Low Risk',
                    np.where(y_prob < 0.7, 'Medium Risk', 'High Risk'))

# Create a DataFrame for results
results = pd.DataFrame({'PD': y_prob, 'Risk Category': risk_categories})
print(results.head())

from sklearn.model_selection import cross_val_score

# Cross-Validation
cv_scores = cross_val_score(rf_clf, X_train_resampled, y_train_resampled, cv=5, scoring='roc_auc')
print("Cross-validation AUC-ROC:", np.mean(cv_scores))

import joblib

# Save the model using joblib
joblib.dump(rf_clf, 'credit_risk_model.pkl')

# Example: Download the model file in Colab
from google.colab import files
files.download('credit_risk_model.pkl')